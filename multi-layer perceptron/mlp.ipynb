{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "     \n",
    "    def __init__(self, input_size, output_size): \n",
    "        \n",
    "        self.inputs = None \n",
    "        # self.weight = np.random.randn(output_size, input_size)\n",
    "        self.weight = np.random.uniform(-10, 10, (output_size, input_size))\n",
    "        # self.bias = np.zeros(shape=(output_size, 1))\n",
    "        self.bias = np.random.rand(output_size, 1)\n",
    "\n",
    "        self.output = None \n",
    "        \n",
    "        # derivived instance field \n",
    "        self.d_bias = None \n",
    "        self.d_weight = None \n",
    "        self.d_output = None \n",
    "        self.d_input = None \n",
    "        \n",
    "    def forward_pass(self, input):\n",
    "        self.inputs = input \n",
    "        output = np.dot(self.weight , self.inputs) + self.bias\n",
    "        \n",
    "        # Clipping the output so that it doesn't overflow when passed into an activation function\n",
    "        self.output = np.round(output, 5)\n",
    "        return self.output\n",
    "        \n",
    "\n",
    "    def backward_pass(self, d_values): \n",
    "        \n",
    "        self.d_weight = np.dot(self.inputs.T)\n",
    "        self.d_bias \n",
    "        self.d_inputs\n",
    "        pass\n",
    "        \n",
    "    def show_attribute(self): \n",
    "        print(\"Input  : \\n\", self.inputs)\n",
    "        print(\"Weight : \\n\", self.weight)\n",
    "        print(\"Biases : \\n\", self.bias) \n",
    "        print(\"Output : \\n\", self.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Activation_Function: \n",
    "    \n",
    "    # Passing in the array/s into the function  \n",
    "    def tanh_activation_function(self, input): \n",
    "        output = np.round(((np.exp(input)) - np.exp(-input)) / ((np.exp(input)) + np.exp(-input)), 5)\n",
    "        return output \n",
    "    \n",
    "    def d_tanh_activation_function(self, input): \n",
    "        output = 1 - ((np.exp(input) - np.exp(-input))**2 / (np.exp(input) + np.exp(-input))**2) \n",
    "        return output \n",
    "    \n",
    "    def sigmoid_activation_function(self, input): \n",
    "        output = 1/(1 + np.exp(-(input)))\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Binary_Cross_Entropy: \n",
    "    \n",
    "    def binary_cross_entropy(self, y_pred, y_true):\n",
    "        \n",
    "        # Calculate binary cross entropy loss\n",
    "        self.loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        \n",
    "        return self.loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mean_Squared_Error: \n",
    "\n",
    "    def mean_squared_error(self, y_pred, y_true):\n",
    "        \n",
    "        if y_pred.ndim != 1: \n",
    "            summed_matrix = y_pred.sum(axis=0)\n",
    "        else:\n",
    "            summed_matrix = y_pred \n",
    "        self.loss = np.mean((y_true - summed_matrix)**2)\n",
    "         \n",
    "        return  self.loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stocastic_Gradient_Descent: \n",
    "    \n",
    "    # By default the learning rate is set to one unless its specified \n",
    "    def __init__(self, learning_rate=1.0 ):\n",
    "        self.learning_rate = learning_rate \n",
    "        \n",
    "    def update_parameters(self, layer): \n",
    "        layer.weight += -self.learning_rate * layer.d_weight \n",
    "        layer.bias += -self.learning_rate * layer.d_bias \n",
    "    \n",
    "    def forward(self): \n",
    "        pass \n",
    "    \n",
    "    def backward(self): \n",
    "        pass \n",
    "\n",
    "        d_loss_output = -2 * (y_true - self.output) \n",
    "        d_output_weighted_sum = 1 - ((np.exp(self.output) - np.exp(-self.output))**2 / (np.exp(self.output) + np.exp(-self.output))**2) \n",
    "        \n",
    "        self.d_weight = np.dot((d_loss_output * d_output_weighted_sum ).reshape(-1, 1), self.inputs.reshape(1, -1))\n",
    "        self.d_bias = d_loss_output * d_output_weighted_sum\n",
    "\n",
    "        return self.d_weight, self.d_bias\n",
    "    \n",
    "    def gradient_descent(self, learning_rate=0.01): \n",
    "        self.weight -= learning_rate * self.d_weight \n",
    "        self.bias -= learning_rate * self.d_bias.reshape(-1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a mini MLP, 3 layers only\n",
    "\n",
    "y_true = [1, 0, 1, 1]\n",
    "x_input = [1], [1], [0], [1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = Layer(4, 4) \n",
    "hidden_layer = Layer(4, 4) \n",
    "output_layer = Layer(4, 1) \n",
    "\n",
    "act_func = Activation_Function() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input  : \n",
      " ([1], [1], [0], [1])\n",
      "Weight : \n",
      " [[-2.50919762  9.01428613  4.63987884  1.97316968]\n",
      " [-6.87962719 -6.88010959 -8.83832776  7.32352292]\n",
      " [ 2.02230023  4.16145156 -9.58831011  9.39819704]\n",
      " [ 6.64885282 -5.75321779 -6.36350066 -6.3319098 ]]\n",
      "Biases : \n",
      " [[0.30424224]\n",
      " [0.52475643]\n",
      " [0.43194502]\n",
      " [0.29122914]]\n",
      "Output : \n",
      " [[ 8.7825 ]\n",
      " [-5.91146]\n",
      " [16.01389]\n",
      " [-5.14505]]\n"
     ]
    }
   ],
   "source": [
    "input_layer.forward_pass(x_input) \n",
    "input_layer.show_attribute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.69430426, 0.72064233, 0.64604057, 0.48464483],\n",
       "       [0.35974525, 0.30746021, 0.73239932, 0.20584258],\n",
       "       [0.03613184, 0.21880737, 0.1259668 , 0.08292977],\n",
       "       [0.87683242, 0.62078701, 0.86156385, 0.79909377]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_func.sigmoid_activation_function(input_layer.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [1.0, 1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, 1.0, 1.0]\n",
    "\n",
    "x_input = [ 1.0, -1.0,  1.0,  1.0,  1.0,  1.0, -1.0,  1.0,  1.0, -1.0], [-1.0, -1.0,  1.0, -1.0, -1.0, -1.0,  1.0,  1.0, -1.0, -1.0], [ 1.0,  1.0, -1.0,  1.0, -1.0,  1.0, -1.0,  1.0, -1.0,  1.0] , [ 1.0, -1.0,  1.0, -1.0,  1.0, -1.0, -1.0,  1.0, -1.0,  1.0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer(input vector, output vector)\n",
    "input_layer = Layer(4, 12)\n",
    "hidden_layer_1 = Layer(12, 12) \n",
    "hidden_layer_2 = Layer(12, 8) \n",
    "hidden_layer_3 = Layer(8, 6)\n",
    "output_layer = Layer(6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Necessary Objects \n",
    "activation = Activation_Function()\n",
    "loss_activation = Binary_Cross_Entropy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By calling the activation function on the ouput of the current layer it updates the output. \n",
    "input_layer.forward_pass(x_input)\n",
    "input_layer.output = activation.sigmoid_activation_function(input_layer.output) \n",
    "\n",
    "hidden_layer_1.forward_pass(input_layer.output)\n",
    "hidden_layer_1.output = activation.sigmoid_activation_function(hidden_layer_1.output)\n",
    "\n",
    "hidden_layer_2.forward_pass(hidden_layer_1.output)\n",
    "hidden_layer_2.output = activation.sigmoid_activation_function(hidden_layer_2.output)\n",
    "\n",
    "hidden_layer_3.forward_pass(hidden_layer_2.output)\n",
    "hidden_layer_3.output = activation.sigmoid_activation_function(hidden_layer_3.output) \n",
    "\n",
    "output_layer.forward_pass(hidden_layer_3.output)\n",
    "output_layer.output = activation.sigmoid_activation_function(output_layer.output) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7458197 , 0.74333975, 0.74187944, 0.74439338, 0.7435057 ,\n",
       "        0.74439338, 0.74131988, 0.74324244, 0.7477598 , 0.745723  ],\n",
       "       [0.77618655, 0.76020249, 0.77991824, 0.76286654, 0.78092073,\n",
       "        0.76286654, 0.76656038, 0.76512037, 0.78210062, 0.77112421],\n",
       "       [0.13465993, 0.14367453, 0.13283978, 0.1411562 , 0.13217194,\n",
       "        0.1411562 , 0.14264904, 0.13843615, 0.13574145, 0.13727379],\n",
       "       [0.8168874 , 0.81355542, 0.81295857, 0.8111592 , 0.81478093,\n",
       "        0.8111592 , 0.80969352, 0.81244712, 0.81071151, 0.818775  ]])"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the final output would be viewed as predictions, we will convert all negatives to 0 and all positives to 1\n",
    "processed_array = np.where(output_layer.output > 0, 1, 0)\n",
    "summed_array = np.sum(processed_array, axis=0)\n",
    "bool_array = np.where((summed_array == 0) | (summed_array == 1), False, True)\n",
    "accuracy = np.mean(bool_array == y_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.7458197 , 0.74333975, 0.74187944, 0.74439338, 0.7435057 ,\n",
       "        0.74439338, 0.74131988, 0.74324244, 0.7477598 , 0.745723  ],\n",
       "       [0.77618655, 0.76020249, 0.77991824, 0.76286654, 0.78092073,\n",
       "        0.76286654, 0.76656038, 0.76512037, 0.78210062, 0.77112421],\n",
       "       [0.13465993, 0.14367453, 0.13283978, 0.1411562 , 0.13217194,\n",
       "        0.1411562 , 0.14264904, 0.13843615, 0.13574145, 0.13727379],\n",
       "       [0.8168874 , 0.81355542, 0.81295857, 0.8111592 , 0.81478093,\n",
       "        0.8111592 , 0.80969352, 0.81244712, 0.81071151, 0.818775  ]])"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.29302968, -0.29705923, -0.29840604, -0.29571424, -0.29571424,\n",
       "        -0.29571424, -0.29975465, -0.29705923, -0.2903523 , -0.29302968],\n",
       "       [-0.25360276, -0.27443685, -0.24846136, -0.27049725, -0.24718013,\n",
       "        -0.27049725, -0.26526848, -0.26787945, -0.24590054, -0.26006691],\n",
       "       [-2.0024805 , -1.93794198, -2.01740615, -1.95899539, -2.02495336,\n",
       "        -1.95899539, -1.94491065, -1.98050159, -1.99510039, -1.98777435],\n",
       "       [-0.20211618, -0.20579491, -0.20702417, -0.20948722, -0.20456717,\n",
       "        -0.20948722, -0.21072103, -0.20825494, -0.20948722, -0.1996712 ]])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(np.round(output_layer.output,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'int' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[320], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Calling a function to compute the loss \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loss \u001b[39m=\u001b[39m loss_activation\u001b[39m.\u001b[39mbinary_cross_entropy(output_layer\u001b[39m.\u001b[39moutput, y_true)\n\u001b[1;32m      4\u001b[0m \u001b[39m# Here we are finding the accuracy. \u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# Since the final output would be viewed as probability, we will essentially convert all negatives to false and all positives to true\u001b[39;00m\n\u001b[1;32m      6\u001b[0m processed_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(output_layer\u001b[39m.\u001b[39moutput \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[306], line 6\u001b[0m, in \u001b[0;36mBinary_Cross_Entropy.binary_cross_entropy\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbinary_cross_entropy\u001b[39m(\u001b[39mself\u001b[39m, y_pred, y_true):\n\u001b[1;32m      4\u001b[0m     \n\u001b[1;32m      5\u001b[0m     \u001b[39m# Calculate binary cross entropy loss\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39mmean(y_true \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlog(y_pred) \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m y_true) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m y_pred))\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'int' and 'list'"
     ]
    }
   ],
   "source": [
    "# Calling a function to compute the loss \n",
    "loss = loss_activation.binary_cross_entropy(output_layer.output, y_true)\n",
    "\n",
    "# Here we are finding the accuracy. \n",
    "# Since the final output would be viewed as probability, we will essentially convert all negatives to false and all positives to true\n",
    "processed_array = np.where(output_layer.output > 0, 1, 0)\n",
    "summed_array = np.sum(processed_array, axis=0)\n",
    "bool_array = np.where((summed_array == 0) | (summed_array == 1), False, True)\n",
    "accuracy = np.mean(bool_array == y_true)\n",
    "    \n",
    "print(\"Loss: \", loss) \n",
    "print(\"Accuracy: \", accuracy)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Stocastic_Gradient_Descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_activation.backward(output_layer.output, layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'float' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[208], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m sgd \u001b[39m=\u001b[39m Stocastic_Gradient_Descent()\n\u001b[0;32m----> 3\u001b[0m sgd\u001b[39m.\u001b[39mupdate_parameters(input_layer)\n\u001b[1;32m      4\u001b[0m sgd\u001b[39m.\u001b[39mupdate_parameters(hidden_layer_1)\n\u001b[1;32m      5\u001b[0m sgd\u001b[39m.\u001b[39mupdate_parameters(hidden_layer_2)\n",
      "Cell \u001b[0;32mIn[200], line 8\u001b[0m, in \u001b[0;36mStocastic_Gradient_Descent.update_parameters\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_parameters\u001b[39m(\u001b[39mself\u001b[39m, layer): \n\u001b[0;32m----> 8\u001b[0m     layer\u001b[39m.\u001b[39mweight \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate \u001b[39m*\u001b[39m layer\u001b[39m.\u001b[39md_weight \n\u001b[1;32m      9\u001b[0m     layer\u001b[39m.\u001b[39mbias \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate \u001b[39m*\u001b[39m layer\u001b[39m.\u001b[39md_bias\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'float' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "optimizer.update_parameters(input_layer)\n",
    "optimizer.update_parameters(hidden_layer_1)\n",
    "optimizer.update_parameters(hidden_layer_2)\n",
    "optimizer.update_parameters(hidden_layer_3)\n",
    "optimizer.update_parameters(output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epochs in range(100): \n",
    "    print(\"step \", epochs, \". Loss: \", \", Accuracy: \" )\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
