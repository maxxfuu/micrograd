{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "     \n",
    "    def __init__(self, input_size, output_size): \n",
    "        \n",
    "        self.inputs = None \n",
    "        self.weight = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.zeros(shape=(output_size, 1))\n",
    "        self.output = None \n",
    "        \n",
    "        # derivived instance field \n",
    "        self.d_bias = None \n",
    "        self.d_weight = None \n",
    "        self.d_output = None \n",
    "        self.d_input = None \n",
    "        \n",
    "    def forward_pass(self, input):\n",
    "        self.inputs = input \n",
    "        output = np.dot(self.weight , self.inputs) + self.bias\n",
    "        \n",
    "        # Clipping the output so that it doesn't overflow when passed into an activation function\n",
    "        self.output = np.round(output, 5)\n",
    "        return self.output\n",
    "        \n",
    "\n",
    "    def backward_pass(self, d_values): \n",
    "        \n",
    "        self.d_weight = np.dot(self.inputs.T)\n",
    "        self.d_bias \n",
    "        self.d_inputs\n",
    "        pass\n",
    "        \n",
    "    def show_attribute(self): \n",
    "        print(\"Input  : \\n\", self.inputs)\n",
    "        print(\"Weight : \\n\", self.weight)\n",
    "        print(\"Biases : \\n\", self.bias) \n",
    "        print(\"Output : \\n\", self.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Function: \n",
    "    \n",
    "    def tanh_activation_function(self, input): \n",
    "        output = np.round(((np.exp(input)) - np.exp(-input)) / ((np.exp(input)) + np.exp(-input)), 5)\n",
    "        return output \n",
    "    \n",
    "    def d_tanh_activation_function(self, input): \n",
    "        output = 1 - ((np.exp(self.output) - np.exp(-self.output))**2 / (np.exp(self.output) + np.exp(-self.output))**2) \n",
    "        return output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Mean_Squared_Error: \n",
    "    \n",
    "    def __init__(self):\n",
    "        pass \n",
    "    \n",
    "    def mean_squared_error(self, y_pred, y_true):\n",
    "        if y_pred.ndim != 1: \n",
    "            summed_matrix = y_pred.sum(axis=0)\n",
    "        else:\n",
    "            summed_matrix = y_pred \n",
    "        self.loss = np.mean((y_true - summed_matrix)**2) \n",
    "        return  self.loss\n",
    "    \n",
    "    def forward(self): \n",
    "        pass \n",
    "    \n",
    "    def backward(self): \n",
    "        pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stocastic_Gradient_Descent: \n",
    "    \n",
    "    # By default the learning rate is set to one unless its specified \n",
    "    def __init__(self, learning_rate=1.0 ):\n",
    "        self.learning_rate = learning_rate \n",
    "        \n",
    "    def update_parameters(self, layer): \n",
    "        layer.weight += -self.learning_rate * layer.d_weight \n",
    "        layer.bias += -self.learning_rate * layer.d_bias \n",
    "    \n",
    "    def forward(self): \n",
    "        pass \n",
    "    \n",
    "    def backward(self): \n",
    "        pass \n",
    "\n",
    "        d_loss_output = -2 * (y_true - self.output) \n",
    "        d_output_weighted_sum = 1 - ((np.exp(self.output) - np.exp(-self.output))**2 / (np.exp(self.output) + np.exp(-self.output))**2) \n",
    "        \n",
    "        self.d_weight = np.dot((d_loss_output * d_output_weighted_sum ).reshape(-1, 1), self.inputs.reshape(1, -1))\n",
    "        self.d_bias = d_loss_output * d_output_weighted_sum\n",
    "\n",
    "        return self.d_weight, self.d_bias\n",
    "    \n",
    "    def gradient_descent(self, learning_rate=0.01): \n",
    "        self.weight -= learning_rate * self.d_weight \n",
    "        self.bias -= learning_rate * self.d_bias.reshape(-1, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [1.0, 2.0, 3.0, 4.0]\n",
    "x_input = [-1.0, 7.0, 2.0, -4.0], [-1.0, 6.0, 9.0, -4.0], [-2.0, -3.0, 2.0, -5.0] , [7.0, 3.0, 2.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer(input, output size)\n",
    "input_layer = Layer(4, 6)\n",
    "hidden_layer_1 = Layer(6, 12) \n",
    "hidden_layer_2 = Layer(12, 8) \n",
    "hidden_layer_3 = Layer(8, 6)\n",
    "output_layer = Layer(6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Necessary Objects \n",
    "activation = Activation_Function()\n",
    "loss = Mean_Squared_Error() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/80/yxql_m2n3b93pljkxwjyygr40000gn/T/ipykernel_66010/4008108341.py:7: RuntimeWarning: overflow encountered in exp\n",
      "  output = np.round(((np.exp(input)) - np.exp(-input)) / ((np.exp(input)) + np.exp(-input)), 5)\n",
      "/var/folders/80/yxql_m2n3b93pljkxwjyygr40000gn/T/ipykernel_66010/4008108341.py:7: RuntimeWarning: invalid value encountered in divide\n",
      "  output = np.round(((np.exp(input)) - np.exp(-input)) / ((np.exp(input)) + np.exp(-input)), 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1., -1.,  1., -1.],\n",
       "       [ 1.,  1., -1.,  1.],\n",
       "       [ 1.,  1., -1.,  1.],\n",
       "       [ 1., -1., nan,  1.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# By calling the activation function on the ouput of the current layer it updates the output. \n",
    "input_layer.forward_pass(x_input)\n",
    "activation.tanh_activation_function(input_layer.output) \n",
    "\n",
    "hidden_layer_1.forward_pass(input_layer.output)\n",
    "activation.tanh_activation_function(hidden_layer_1.output)\n",
    "\n",
    "hidden_layer_2.forward_pass(hidden_layer_1.output)\n",
    "activation.tanh_activation_function(hidden_layer_2.output)\n",
    "\n",
    "hidden_layer_3.forward_pass(hidden_layer_2.output)\n",
    "activation.tanh_activation_function(hidden_layer_3.output) \n",
    "\n",
    "output_layer.forward_pass(hidden_layer_3.output)\n",
    "activation.tanh_activation_function(output_layer.output) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-307.47421, -222.39564,   77.03167, -164.65518],\n",
       "       [ 320.81262,  335.74711, -493.81425,  103.32517],\n",
       "       [ 427.79034,  329.07085, -212.20346,  186.35884],\n",
       "       [  52.5283 ,  -38.67137, -758.0334 ,   33.80043]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.float64' object has no attribute 'mean_squared_error'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[76], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m compute_loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean_squared_error(hidden_layer_3\u001b[39m.\u001b[39moutput, y_true)\n\u001b[1;32m      2\u001b[0m compute_loss\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'mean_squared_error'"
     ]
    }
   ],
   "source": [
    "\n",
    "compute_loss = loss.mean_squared_error(hidden_layer_3.output, y_true)\n",
    "compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = Stocastic_Gradient_Descent()\n",
    "\n",
    "sgd.update_parameters(input_layer)\n",
    "sgd.update_parameters(hidden_layer_1)\n",
    "sgd.update_parameters(hidden_layer_2)\n",
    "sgd.update_parameters(hidden_layer_3)\n",
    "sgd.update_parameters(output_layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epochs in range(100): \n",
    "    print(\"step \", epochs, \". Loss: \", \", Accuracy: \" )\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
