{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Layer:\n",
    "     \n",
    "    def __init__(self, input_size, output_size): \n",
    "        \n",
    "        self.inputs = None \n",
    "        self.weight = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.zeros(shape=(output_size, 1))\n",
    "        self.output = None \n",
    "        \n",
    "        # derivived instance field \n",
    "        self.d_bias = None \n",
    "        self.d_weight = None \n",
    "    \n",
    "    def show_attribute(self): \n",
    "        print(\"Input  : \\n\", self.inputs)\n",
    "        print(\"Weight : \\n\", self.weight)\n",
    "        print(\"Biases : \\n\", self.bias) \n",
    "        print(\"Output : \\n\", self.output)\n",
    "        \n",
    "    def tanh_activation_function(self, input): \n",
    "        output = ((np.exp(input)) - np.exp(-input)) / ((np.exp(input)) + np.exp(-input)) \n",
    "        return output \n",
    "        \n",
    "    def forward_pass(self, input):\n",
    "        self.inputs = input \n",
    "        weighted_sum = np.dot(self.weight , self.inputs) + self.bias \n",
    "        self.output = self.tanh_activation_function(weighted_sum)\n",
    "        return self.output\n",
    "    \n",
    "    def mean_squared_error(self, y_true):\n",
    "        if self.output.ndim != 1: \n",
    "            summed_matrix = self.output.sum(axis=0)\n",
    "        self.loss = np.mean((y_true - summed_matrix)**2) \n",
    "        return  self.loss\n",
    "    \n",
    "    # Functions that calculates the derivative \n",
    "    \n",
    "    def d_tanh_activation_function(self, input): \n",
    "        output = 1 - ((np.exp(self.output) - np.exp(-self.output))**2 / (np.exp(self.output) + np.exp(-self.output))**2) \n",
    "        return output \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def backward_pass(self, y_true): \n",
    "        \n",
    "        self.d_weight \n",
    "        self.d_bias \n",
    "        self.d_inputs\n",
    "        \n",
    "        \n",
    "        # mse gradient \n",
    "        d_loss_output = -2 * (y_true - self.output) \n",
    "        d_output_weighted_sum = 1 - ((np.exp(self.output) - np.exp(-self.output))**2 / (np.exp(self.output) + np.exp(-self.output))**2) \n",
    "        \n",
    "        self.d_weight = np.dot((d_loss_output * d_output_weighted_sum ).reshape(-1, 1), self.inputs.reshape(1, -1))\n",
    "        self.d_bias = d_loss_output * d_output_weighted_sum\n",
    "\n",
    "        return self.d_weight, self.d_bias\n",
    "    \n",
    "    def gradient_descent(self, learning_rate=0.01): \n",
    "        self.weight -= learning_rate * self.d_weight \n",
    "        self.bias -= learning_rate * self.d_bias.reshape(-1, 1) \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [1.0, 2.0, 3.0, 4.0]\n",
    "x_input = [-1.0, 7.0, 2.0, -4.0], [-1.0, 6.0, 9.0, -4.0], [-2.0, -3.0, 2.0, -5.0] , [7.0, 3.0, 2.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer(input, output size)\n",
    "input_layer = Layer(4, 6)\n",
    "hidden_layer_1 = Layer(6, 12) \n",
    "hidden_layer_2 = Layer(12, 8) \n",
    "hidden_layer_3 = Layer(8, 6)\n",
    "output_Layer = Layer(6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.90398318,  0.09843332, -0.8302713 ,  0.62256749],\n",
       "       [-0.75009054,  0.99356287, -0.53506272, -0.80313296],\n",
       "       [ 0.99962352,  0.98841835,  0.99769346, -0.94078276],\n",
       "       [-0.47542198,  0.75625597,  0.96847247, -0.9989815 ]])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1 = input_layer.forward_pass(x_input)\n",
    "output_2 = hidden_layer_1.forward_pass(output_1)\n",
    "output_3 = hidden_layer_2.forward_pass(output_2)\n",
    "output_4 = hidden_layer_3.forward_pass(output_3)\n",
    "final_output = output_Layer.forward_pass(output_4)\n",
    "final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.4584802120050973"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = output_Layer.mean_squared_error(final_output)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0 . Loss:  , Accuracy: \n",
      "step  1 . Loss:  , Accuracy: \n",
      "step  2 . Loss:  , Accuracy: \n",
      "step  3 . Loss:  , Accuracy: \n",
      "step  4 . Loss:  , Accuracy: \n",
      "step  5 . Loss:  , Accuracy: \n",
      "step  6 . Loss:  , Accuracy: \n",
      "step  7 . Loss:  , Accuracy: \n",
      "step  8 . Loss:  , Accuracy: \n",
      "step  9 . Loss:  , Accuracy: \n",
      "step  10 . Loss:  , Accuracy: \n",
      "step  11 . Loss:  , Accuracy: \n",
      "step  12 . Loss:  , Accuracy: \n",
      "step  13 . Loss:  , Accuracy: \n",
      "step  14 . Loss:  , Accuracy: \n",
      "step  15 . Loss:  , Accuracy: \n",
      "step  16 . Loss:  , Accuracy: \n",
      "step  17 . Loss:  , Accuracy: \n",
      "step  18 . Loss:  , Accuracy: \n",
      "step  19 . Loss:  , Accuracy: \n",
      "step  20 . Loss:  , Accuracy: \n",
      "step  21 . Loss:  , Accuracy: \n",
      "step  22 . Loss:  , Accuracy: \n",
      "step  23 . Loss:  , Accuracy: \n",
      "step  24 . Loss:  , Accuracy: \n",
      "step  25 . Loss:  , Accuracy: \n",
      "step  26 . Loss:  , Accuracy: \n",
      "step  27 . Loss:  , Accuracy: \n",
      "step  28 . Loss:  , Accuracy: \n",
      "step  29 . Loss:  , Accuracy: \n",
      "step  30 . Loss:  , Accuracy: \n",
      "step  31 . Loss:  , Accuracy: \n",
      "step  32 . Loss:  , Accuracy: \n",
      "step  33 . Loss:  , Accuracy: \n",
      "step  34 . Loss:  , Accuracy: \n",
      "step  35 . Loss:  , Accuracy: \n",
      "step  36 . Loss:  , Accuracy: \n",
      "step  37 . Loss:  , Accuracy: \n",
      "step  38 . Loss:  , Accuracy: \n",
      "step  39 . Loss:  , Accuracy: \n",
      "step  40 . Loss:  , Accuracy: \n",
      "step  41 . Loss:  , Accuracy: \n",
      "step  42 . Loss:  , Accuracy: \n",
      "step  43 . Loss:  , Accuracy: \n",
      "step  44 . Loss:  , Accuracy: \n",
      "step  45 . Loss:  , Accuracy: \n",
      "step  46 . Loss:  , Accuracy: \n",
      "step  47 . Loss:  , Accuracy: \n",
      "step  48 . Loss:  , Accuracy: \n",
      "step  49 . Loss:  , Accuracy: \n",
      "step  50 . Loss:  , Accuracy: \n",
      "step  51 . Loss:  , Accuracy: \n",
      "step  52 . Loss:  , Accuracy: \n",
      "step  53 . Loss:  , Accuracy: \n",
      "step  54 . Loss:  , Accuracy: \n",
      "step  55 . Loss:  , Accuracy: \n",
      "step  56 . Loss:  , Accuracy: \n",
      "step  57 . Loss:  , Accuracy: \n",
      "step  58 . Loss:  , Accuracy: \n",
      "step  59 . Loss:  , Accuracy: \n",
      "step  60 . Loss:  , Accuracy: \n",
      "step  61 . Loss:  , Accuracy: \n",
      "step  62 . Loss:  , Accuracy: \n",
      "step  63 . Loss:  , Accuracy: \n",
      "step  64 . Loss:  , Accuracy: \n",
      "step  65 . Loss:  , Accuracy: \n",
      "step  66 . Loss:  , Accuracy: \n",
      "step  67 . Loss:  , Accuracy: \n",
      "step  68 . Loss:  , Accuracy: \n",
      "step  69 . Loss:  , Accuracy: \n",
      "step  70 . Loss:  , Accuracy: \n",
      "step  71 . Loss:  , Accuracy: \n",
      "step  72 . Loss:  , Accuracy: \n",
      "step  73 . Loss:  , Accuracy: \n",
      "step  74 . Loss:  , Accuracy: \n",
      "step  75 . Loss:  , Accuracy: \n",
      "step  76 . Loss:  , Accuracy: \n",
      "step  77 . Loss:  , Accuracy: \n",
      "step  78 . Loss:  , Accuracy: \n",
      "step  79 . Loss:  , Accuracy: \n",
      "step  80 . Loss:  , Accuracy: \n",
      "step  81 . Loss:  , Accuracy: \n",
      "step  82 . Loss:  , Accuracy: \n",
      "step  83 . Loss:  , Accuracy: \n",
      "step  84 . Loss:  , Accuracy: \n",
      "step  85 . Loss:  , Accuracy: \n",
      "step  86 . Loss:  , Accuracy: \n",
      "step  87 . Loss:  , Accuracy: \n",
      "step  88 . Loss:  , Accuracy: \n",
      "step  89 . Loss:  , Accuracy: \n",
      "step  90 . Loss:  , Accuracy: \n",
      "step  91 . Loss:  , Accuracy: \n",
      "step  92 . Loss:  , Accuracy: \n",
      "step  93 . Loss:  , Accuracy: \n",
      "step  94 . Loss:  , Accuracy: \n",
      "step  95 . Loss:  , Accuracy: \n",
      "step  96 . Loss:  , Accuracy: \n",
      "step  97 . Loss:  , Accuracy: \n",
      "step  98 . Loss:  , Accuracy: \n",
      "step  99 . Loss:  , Accuracy: \n"
     ]
    }
   ],
   "source": [
    "for epochs in range(100): \n",
    "    print(\"step \", epochs, \". Loss: \", \", Accuracy: \" )\n",
    "    pass \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
