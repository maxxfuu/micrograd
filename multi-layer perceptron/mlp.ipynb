{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (4104724600.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[206], line 13\u001b[0;36m\u001b[0m\n\u001b[0;31m    return print(layer_output\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "class MLP: \n",
    "    \n",
    "    def __init__(self, batch_size_inputs): \n",
    "        self.layer = None\n",
    "        self.batch_size_inputs = batch_size_inputs \n",
    "        \n",
    "    def forward_pass(self):\n",
    "        self.layer = self.batch_size_inputs \n",
    "        layer_output = []\n",
    "        for layers in self.batch_size_inputs: \n",
    "            layer_output.append(Layer.forward_pass(layers))\n",
    "        \n",
    "        return print(layer_output)\n",
    "\n",
    "class Layer:\n",
    "     \n",
    "    def __init__(self, input_size, output_size): \n",
    "        \n",
    "        self.inputs = None \n",
    "        self.weight = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.zeros(shape=(output_size, 1))\n",
    "        self.output = None \n",
    "        \n",
    "        # derivived instance field \n",
    "        self.d_bias = None \n",
    "        self.d_weight = None \n",
    "    \n",
    "    def show_attribute(self): \n",
    "        print(\"Input  : \\n\", self.inputs)\n",
    "        print(\"Weight : \\n\", self.weight)\n",
    "        print(\"Biases : \\n\", self.bias) \n",
    "        print(\"Output : \\n\", self.output)\n",
    "        \n",
    "    def tanh_activation_function(self, input): \n",
    "        output = ((np.exp(input)) - np.exp(-input)) / ((np.exp(input)) + np.exp(-input)) \n",
    "        return output \n",
    "        \n",
    "    def forward_pass(self, input):\n",
    "        self.inputs = input \n",
    "        weighted_sum = np.dot(self.weight , self.inputs) + self.bias \n",
    "        # self.output = self.tanh_activation_function(weighted_sum)\n",
    "        \n",
    "        self.output = weighted_sum\n",
    "        return self.output\n",
    "        \n",
    "        # return self.output.flatten() \n",
    "    \n",
    "    def mean_squared_error(self, y_true):\n",
    "        self.loss = np.mean((y_true - self.output)**2)\n",
    "        print(y_true - self.output)\n",
    "        print((y_true - self.output)**2) \n",
    "        return  self.loss\n",
    "    \n",
    "    def backward_pass(self, y_true): # Compute the loss function with respect to each weight. \n",
    "        d_loss_output = -2 * (y_true - self.output) \n",
    "        d_output_weighted_sum = 1 - ((np.exp(self.output) - np.exp(-self.output))**2 / (np.exp(self.output) + np.exp(-self.output))**2) \n",
    "        \n",
    "        self.d_weight = np.dot((d_loss_output * d_output_weighted_sum ).reshape(-1, 1), self.inputs.reshape(1, -1))\n",
    "        self.d_bias = d_loss_output * d_output_weighted_sum\n",
    "\n",
    "        return self.d_weight, self.d_bias\n",
    "    \n",
    "    def gradient_descent(self, learning_rate=0.01): \n",
    "        self.weight -= learning_rate * self.d_weight \n",
    "        self.bias -= learning_rate * self.d_bias.reshape(-1, 1) \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [1.0, 2.0, 3.0, 4.0]\n",
    "x_input = [-1.0, 7.0, 2.0, -4.0], [-1.0, 6.0, 9.0, -4.0], [-2.0, -3.0, 2.0, -5.0] , [7.0, 3.0, 2.0, 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Layer.forward_pass() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[205], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m output_Layer \u001b[39m=\u001b[39m Layer(\u001b[39m7\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[1;32m      6\u001b[0m mlp \u001b[39m=\u001b[39m MLP(x_input)\n\u001b[0;32m----> 8\u001b[0m mlp\u001b[39m.\u001b[39mforward_pass()\n",
      "Cell \u001b[0;32mIn[203], line 11\u001b[0m, in \u001b[0;36mMLP.forward_pass\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      9\u001b[0m layer_output \u001b[39m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[39mfor\u001b[39;00m layers \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size_inputs : \n\u001b[0;32m---> 11\u001b[0m     layer_output\u001b[39m.\u001b[39mappend(Layer\u001b[39m.\u001b[39mforward_pass(layers))\n\u001b[1;32m     13\u001b[0m \u001b[39mreturn\u001b[39;00m layer_output\n",
      "\u001b[0;31mTypeError\u001b[0m: Layer.forward_pass() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "input_layer = Layer(4, 6)\n",
    "hidden_layer_1 = Layer(6, 12) \n",
    "hidden_layer_2 = Layer(12, 7) \n",
    "output_Layer = Layer(7, 4)\n",
    "\n",
    "mlp = MLP(x_input)\n",
    "\n",
    "mlp.forward_pass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'T'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[180], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output_1 \u001b[39m=\u001b[39m input_layer\u001b[39m.\u001b[39mforward_pass(x_input)\n",
      "Cell \u001b[0;32mIn[176], line 20\u001b[0m, in \u001b[0;36mLayer.forward_pass\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward_pass\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m \n\u001b[0;32m---> 20\u001b[0m     weighted_sum \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight , \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs\u001b[39m.\u001b[39mT) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \n\u001b[1;32m     21\u001b[0m     \u001b[39m# self.output = self.tanh_activation_function(weighted_sum)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m weighted_sum\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'T'"
     ]
    }
   ],
   "source": [
    "output_1 = input_layer.forward_pass(x_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-1.0, 7.0, 2.0, -4.0], (6, 4))"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_layer.inputs , input_layer.weight.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "PIVOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[148], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m layer_1_size \u001b[39m=\u001b[39m \u001b[39m7\u001b[39m\n\u001b[1;32m      3\u001b[0m inputs \u001b[39m=\u001b[39m ([\u001b[39m1.0\u001b[39m, \u001b[39m7.0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m9.0\u001b[39m, \u001b[39m2.\u001b[39m\u001b[39m-\u001b[39m\u001b[39m0\u001b[39m])\n\u001b[0;32m----> 4\u001b[0m Layer_1 \u001b[39m=\u001b[39m Layer(layer_1_size, inputs)\n\u001b[1;32m      5\u001b[0m o_L1 \u001b[39m=\u001b[39m Layer_1\u001b[39m.\u001b[39mforward_pass()\n",
      "Cell \u001b[0;32mIn[143], line 6\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, input_size, output_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, input_size, output_size): \n\u001b[1;32m      5\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \n\u001b[0;32m----> 6\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandn(output_size, input_size)\n\u001b[1;32m      7\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(shape\u001b[39m=\u001b[39m(output_size, \u001b[39m1\u001b[39m))\n\u001b[1;32m      8\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:1287\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randn\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:1448\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.standard_normal\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_common.pyx:636\u001b[0m, in \u001b[0;36mnumpy.random._common.cont\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "#Creating an Input Layer of 4 inputs feeding forward into 7 neurons\n",
    "layer_1_size = 7\n",
    "inputs = ([1.0, 7.0, -9.0, 2.-0])\n",
    "Layer_1 = Layer(layer_1_size, inputs)\n",
    "o_L1 = Layer_1.forward_pass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.99883664, -0.98511615, -0.99326288,  0.75660914,  0.71835215,\n",
       "        0.93189981, -0.99927461, -0.99852038,  0.91490985, -0.98437055,\n",
       "       -0.84702683, -0.08111319, -0.18293876, -0.78488994, -0.89663469,\n",
       "       -0.7747573 , -0.99984805,  0.98815708])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_2_size = 18\n",
    "inputs_2 = o_L1\n",
    "Layer_2 = Layer(layer_2_size, o_L1)\n",
    "# Layer_2.show_attribute()\n",
    "o_L2 = Layer_2.forward_pass() \n",
    "o_L2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.28000856, -0.38284641,  0.48349093,  0.76801382,  0.99999288,\n",
       "        0.99832513, -0.8795215 , -0.99858122, -0.51729256, -0.9047006 ,\n",
       "        0.99999524, -0.87681524])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_3_size = 12\n",
    "inputs_3 = o_L2\n",
    "Layer_3 = Layer(layer_3_size, o_L2)\n",
    "# Layer_2.show_attribute()\n",
    "o_L3 = Layer_3.forward_pass() \n",
    "o_L3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.74515171, -0.38589017,  0.97277526,  0.98443264])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_4_size = 4\n",
    "inputs_4 = o_L3\n",
    "Layer_4 = Layer(layer_4_size, o_L3)\n",
    "# Layer_2.show_attribute()\n",
    "o_L4 = Layer_4.forward_pass() \n",
    "o_L4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "print(Layer_4.output == o_L4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25484829 2.38589017 2.02722474 3.01556736]]\n",
      "[[0.06494765 5.69247192 4.10964016 9.09364651]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.740176558349234"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Layer_4.mean_squared_error(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.25484829 2.38589017 2.02722474 3.01556736]]\n",
      "[[0.06494765 5.69247192 4.10964016 9.09364651]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.740176558349234"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we are going to backpropagate, calculating the loss with respect to it each weight. \n",
    "# Then we will use gradient descent as an optimization algorithm used for minimzing the loss function. \n",
    "\n",
    "Layer_4.mean_squared_error(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.08566917,  0.11713262, -0.147925  , -0.23497535, -0.3059498 ,\n",
       "         -0.30543955,  0.26909134,  0.3055179 ,  0.15826668,  0.27679494,\n",
       "         -0.30595052,  0.26826336],\n",
       "        [ 1.15537903,  1.57971144, -1.99499361, -3.16899984, -4.12619822,\n",
       "         -4.11931672,  3.62910588,  4.12037342,  2.13446685,  3.7330006 ,\n",
       "         -4.12620799,  3.61793925],\n",
       "        [ 0.49681922,  0.67928444, -0.85785802, -1.36268703, -1.77428749,\n",
       "         -1.77132841,  1.56053511,  1.7717828 ,  0.91783226,  1.6052104 ,\n",
       "         -1.7742917 ,  1.55573339],\n",
       "        [ 0.72618377,  0.99288698, -1.25390191, -1.99179332, -2.59341574,\n",
       "         -2.58909055,  2.28098114,  2.58975471,  1.34156422,  2.34628149,\n",
       "         -2.59342188,  2.27396264]]),\n",
       " array([[-0.30595198, -4.12622761, -1.77430013, -2.59343421]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_4 = Layer_4.backward_pass(y_true)\n",
    "gradient_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (4,) (1,12) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gradient_3 \u001b[39m=\u001b[39m Layer_3\u001b[39m.\u001b[39mbackward_pass(o_L4)\n\u001b[1;32m      2\u001b[0m gradient_3\n",
      "Cell \u001b[0;32mIn[2], line 29\u001b[0m, in \u001b[0;36mLayer.backward_pass\u001b[0;34m(self, y_true)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbackward_pass\u001b[39m(\u001b[39mself\u001b[39m, y_true): \u001b[39m# Compute the loss function with respect to each weight. \u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     d_loss_output \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (y_true \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput) \n\u001b[1;32m     30\u001b[0m     d_output_weighted_sum \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m ((np\u001b[39m.\u001b[39mexp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput) \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m/\u001b[39m (np\u001b[39m.\u001b[39mexp(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput) \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput))\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m) \n\u001b[1;32m     32\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot((d_loss_output \u001b[39m*\u001b[39m d_output_weighted_sum )\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (4,) (1,12) "
     ]
    }
   ],
   "source": [
    "gradient_3 = Layer_3.backward_pass(o_L4)\n",
    "gradient_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
